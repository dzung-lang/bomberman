{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10183846,"sourceType":"datasetVersion","datasetId":6291126},{"sourceId":10186388,"sourceType":"datasetVersion","datasetId":6292955}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dzungta/tfm-promax?scriptVersionId=212814644\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:48:02.66654Z","iopub.execute_input":"2024-12-13T04:48:02.666864Z","iopub.status.idle":"2024-12-13T04:48:02.671748Z","shell.execute_reply.started":"2024-12-13T04:48:02.666832Z","shell.execute_reply":"2024-12-13T04:48:02.670786Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:48:02.673063Z","iopub.execute_input":"2024-12-13T04:48:02.673325Z","iopub.status.idle":"2024-12-13T04:48:02.681979Z","shell.execute_reply.started":"2024-12-13T04:48:02.6733Z","shell.execute_reply":"2024-12-13T04:48:02.681217Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LayerNormalization","metadata":{}},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n\n    def __init__(self, features: int, eps:float=10**-6) -> None:\n        super().__init__()\n        self.eps = eps\n        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n\n    def forward(self, x):\n        # x: (batch, seq_len, hidden_size)\n         # Keep the dimension for broadcasting\n        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n        # Keep the dimension for broadcasting\n        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n        # eps is to prevent dividing by zero or when std is very small\n        return self.alpha * (x - mean) / (std + self.eps) + self.bias","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:48:02.683704Z","iopub.execute_input":"2024-12-13T04:48:02.684013Z","iopub.status.idle":"2024-12-13T04:48:02.691315Z","shell.execute_reply.started":"2024-12-13T04:48:02.683968Z","shell.execute_reply":"2024-12-13T04:48:02.690513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FeedForwardBlock(nn.Module):\n\n    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n        super().__init__()\n        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n        self.dropout = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n\n    def forward(self, x):\n        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:48:02.692334Z","iopub.execute_input":"2024-12-13T04:48:02.692649Z","iopub.status.idle":"2024-12-13T04:48:02.703579Z","shell.execute_reply.started":"2024-12-13T04:48:02.692624Z","shell.execute_reply":"2024-12-13T04:48:02.702777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class InputEmbeddings(nn.Module):\n\n    def __init__(self, d_model: int, vocab_size: int) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.embedding = nn.Embedding(vocab_size, d_model)\n\n    def forward(self, x):\n        # (batch, seq_len) --> (batch, seq_len, d_model)\n        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n        return self.embedding(x) * math.sqrt(self.d_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:48:02.705065Z","iopub.execute_input":"2024-12-13T04:48:02.705326Z","iopub.status.idle":"2024-12-13T04:48:02.712077Z","shell.execute_reply.started":"2024-12-13T04:48:02.705301Z","shell.execute_reply":"2024-12-13T04:48:02.711381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.seq_len = seq_len\n        self.dropout = nn.Dropout(dropout)\n        # Create a matrix of shape (seq_len, d_model)\n        pe = torch.zeros(seq_len, d_model)\n        # Create a vector of shape (seq_len)\n        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n        # Create a vector of shape (d_model)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n        # Apply sine to even indices\n        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n        # Apply cosine to odd indices\n        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n        # Add a batch dimension to the positional encoding\n        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n        # Register the positional encoding as a buffer\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n        return self.dropout(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:48:02.712916Z","iopub.execute_input":"2024-12-13T04:48:02.713148Z","iopub.status.idle":"2024-12-13T04:48:02.72258Z","shell.execute_reply.started":"2024-12-13T04:48:02.713125Z","shell.execute_reply":"2024-12-13T04:48:02.721718Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MultiHeadAttentionBlock(nn.Module):\n\n    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n        super().__init__()\n        self.d_model = d_model # Embedding vector size\n        self.h = h # Number of heads\n        # Make sure d_model is divisible by h\n        assert d_model % h == 0, \"d_model is not divisible by h\"\n\n        self.d_k = d_model // h # Dimension of vector seen by each head\n        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n        self.dropout = nn.Dropout(dropout)\n\n    @staticmethod\n    def attention(query, key, value, mask, dropout: nn.Dropout):\n        d_k = query.shape[-1]\n        # Just apply the formula from the paper\n        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n        if mask is not None:\n            # Write a very low value (indicating -inf) to the positions where mask == 0\n            attention_scores.masked_fill_(mask == 0, -1e9)\n        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n        if dropout is not None:\n            attention_scores = dropout(attention_scores)\n        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n        # return attention scores which can be used for visualization\n        return (attention_scores @ value), attention_scores\n\n    def forward(self, q, k, v, mask):\n        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n\n        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n\n        # Calculate attention\n        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n        \n        # Combine all the heads together\n        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n\n        # Multiply by Wo\n        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n        return self.w_o(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:48:02.913402Z","iopub.execute_input":"2024-12-13T04:48:02.913744Z","iopub.status.idle":"2024-12-13T04:48:02.924237Z","shell.execute_reply.started":"2024-12-13T04:48:02.913715Z","shell.execute_reply":"2024-12-13T04:48:02.923337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ResidualConnection(nn.Module):\n    \n        def __init__(self, features: int, dropout: float) -> None:\n            super().__init__()\n            self.dropout = nn.Dropout(dropout)\n            self.norm = LayerNormalization(features)\n    \n        def forward(self, x, sublayer):\n            return x + self.dropout(sublayer(self.norm(x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:48:02.92566Z","iopub.execute_input":"2024-12-13T04:48:02.925902Z","iopub.status.idle":"2024-12-13T04:48:02.937825Z","shell.execute_reply.started":"2024-12-13T04:48:02.925878Z","shell.execute_reply":"2024-12-13T04:48:02.937095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n\n    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n\n    def forward(self, x, src_mask):\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n        x = self.residual_connections[1](x, self.feed_forward_block)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:48:02.938916Z","iopub.execute_input":"2024-12-13T04:48:02.939264Z","iopub.status.idle":"2024-12-13T04:48:02.950562Z","shell.execute_reply.started":"2024-12-13T04:48:02.939226Z","shell.execute_reply":"2024-12-13T04:48:02.949693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Encoder(nn.Module):\n\n    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n        super().__init__()\n        self.layers = layers\n        self.norm = LayerNormalization(features)\n\n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:48:02.951598Z","iopub.execute_input":"2024-12-13T04:48:02.951912Z","iopub.status.idle":"2024-12-13T04:48:02.961318Z","shell.execute_reply.started":"2024-12-13T04:48:02.951877Z","shell.execute_reply":"2024-12-13T04:48:02.960654Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\n\n# Reuse the existing components: InputEmbeddings, PositionalEncoding, LayerNormalization, etc.\n# Modify only the components necessary for classification.\n\nclass TransformerEncoderClassifier(nn.Module):\n    def __init__(self, \n                 src_vocab_size: int, \n                 src_seq_len: int, \n                 d_model: int, \n                 N: int, \n                 h: int, \n                 d_ff: int, \n                 num_classes: int, \n                 dropout: float = 0.1):\n        super().__init__()\n        # Input embedding and positional encoding\n        self.src_embed = InputEmbeddings(d_model, src_vocab_size)\n        self.src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n\n        # Transformer encoder\n        encoder_blocks = []\n        for _ in range(N):\n            self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n            feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n            encoder_block = EncoderBlock(d_model, self_attention_block, feed_forward_block, dropout)\n            encoder_blocks.append(encoder_block)\n        self.encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n        \n        # Classification head\n        self.classifier = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model // 2, num_classes)\n        )\n    \n    def forward(self, src, src_mask):\n        # Embed and encode the input\n        src = self.src_embed(src)  # (batch, seq_len, d_model)\n        src = self.src_pos(src)    # (batch, seq_len, d_model)\n        encoder_output = self.encoder(src, src_mask)  # (batch, seq_len, d_model)\n\n        # Aggregate features for classification\n        # Mean pooling over the sequence length\n        pooled_output = encoder_output.mean(dim=1)  # (batch, d_model)\n\n        # Classification\n        return self.classifier(pooled_output)\n\n# Utility to create masks\ndef create_padding_mask(input_tensor, pad_token_idx):\n    return (input_tensor != pad_token_idx).unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, seq_len)\n\n# Hyperparameters and training setup\n# src_vocab_size = 20000  # Adjust based on your dataset's vocabulary size\n# src_seq_len = 512       # Maximum sequence length for a paragraph\n# d_model = 256           # Embedding size\n# N = 6                   # Number of encoder layers\n# h = 8                   # Number of attention heads\n# d_ff = 1024             # Feedforward network size\n# num_classes = 14        # Number of topic classes\n# dropout = 0.1\n\n# # Instantiate the model\n# model = TransformerEncoderClassifier(\n#     src_vocab_size=src_vocab_size,\n#     src_seq_len=src_seq_len,\n#     d_model=d_model,\n#     N=N,\n#     h=h,\n#     d_ff=d_ff,\n#     num_classes=num_classes,\n#     dropout=dropout\n# )\n\n# # Loss and optimizer\n# criterion = nn.CrossEntropyLoss()\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# # Dummy training loop\n# def train_model(model, train_loader, num_epochs=10):\n#     model.train()\n#     for epoch in range(num_epochs):\n#         for batch in train_loader:\n#             paragraphs, labels = batch\n#             optimizer.zero_grad()\n            \n#             # Create masks (assuming 0 is the PAD token)\n#             src_mask = create_padding_mask(paragraphs, pad_token_idx=0)\n            \n#             # Forward pass\n#             outputs = model(paragraphs, src_mask)  # (batch, num_classes)\n            \n#             # Compute loss\n#             loss = criterion(outputs, labels)\n#             loss.backward()\n#             optimizer.step()\n        \n#         print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:48:02.964003Z","iopub.execute_input":"2024-12-13T04:48:02.964321Z","iopub.status.idle":"2024-12-13T04:48:02.973272Z","shell.execute_reply.started":"2024-12-13T04:48:02.964296Z","shell.execute_reply":"2024-12-13T04:48:02.972543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch \nimport torch.nn as nn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:48:02.974132Z","iopub.execute_input":"2024-12-13T04:48:02.974409Z","iopub.status.idle":"2024-12-13T04:48:02.983687Z","shell.execute_reply.started":"2024-12-13T04:48:02.974384Z","shell.execute_reply":"2024-12-13T04:48:02.983055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# stop_words = pd.read_csv('D:/AI/S1_Y3/DL/Code/BTL/data/stopwords.csv')\n# stop_words","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:48:02.984536Z","iopub.execute_input":"2024-12-13T04:48:02.984775Z","iopub.status.idle":"2024-12-13T04:48:02.991957Z","shell.execute_reply.started":"2024-12-13T04:48:02.984752Z","shell.execute_reply":"2024-12-13T04:48:02.991204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/pre-data/preprocessed_data.csv')\ndata ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:48:02.992792Z","iopub.execute_input":"2024-12-13T04:48:02.99303Z","iopub.status.idle":"2024-12-13T04:48:03.233404Z","shell.execute_reply.started":"2024-12-13T04:48:02.993006Z","shell.execute_reply":"2024-12-13T04:48:03.23251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from pyvi.ViTokenizer import tokenize\n# import re, os, string\n# import pandas as pd\n\n# def clean_text(text):\n#     text = re.sub('<.*?>', '', text).strip()\n#     text = re.sub('(\\s)+', r'\\1', text)\n#     return text\n\n# def normalize_text(text):\n#     listpunctuation = string.punctuation.replace('_', '')\n#     for i in listpunctuation:\n#         text = text.replace(i, ' ')\n#     return text.lower()\n\n# # list stopwords\n\n\n# def remove_stopword(text):\n#     pre_text = []\n#     words = text.split()\n#     for word in words:\n#         if word not in stop_words:\n#             pre_text.append(word)\n#     text2 = ' '.join(pre_text)\n\n#     return text2\n\n# def sentence_segment(text):\n#     sents = re.split(\"([.?!])?[\\n]+|[.?!] \", text)\n#     return sents\n\n\n# def word_segment(sent):\n#     sent = tokenizer.tokenize(sent)\n#     return sent\n\n# def preprocess_text(text):\n#     \"\"\"Full preprocessing pipeline for a single text.\"\"\"\n#     text = clean_text(text)\n#     text = normalize_text(text)\n#     text = remove_stopword(text)\n#     return text\n\n# def convert_paragraphs_to_list(dataframe, text_column):\n#     \"\"\"Convert the content of the specified text column to a list of processed strings.\"\"\"\n#     processed_texts = dataframe[text_column].apply(preprocess_text).tolist()\n#     return processed_texts\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:48:03.234613Z","iopub.execute_input":"2024-12-13T04:48:03.235333Z","iopub.status.idle":"2024-12-13T04:48:03.23989Z","shell.execute_reply.started":"2024-12-13T04:48:03.235293Z","shell.execute_reply":"2024-12-13T04:48:03.239097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for i in range(len(data)):\n#     content = clean_text(data.loc[i, 'Paragraph'])\n#     sents = sentence_segment(content)\n#     for sent in sents:\n#         if(sent != None):\n#             sent = word_segment(sent)\n#             sent = remove_stopword(normalize_text(sent))\n#             if(len(sent.split()) > 1):\n#                 data.loc[i, 'Paragraph'] = sent\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:48:03.24096Z","iopub.execute_input":"2024-12-13T04:48:03.241241Z","iopub.status.idle":"2024-12-13T04:48:03.251442Z","shell.execute_reply.started":"2024-12-13T04:48:03.241211Z","shell.execute_reply":"2024-12-13T04:48:03.25061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\ndef build_vocab(tokenized_texts, min_freq=1, special_tokens=[\"<pad>\", \"<unk>\"]):\n    \n    # Flatten the list of tokenized texts and count token frequencies\n    all_tokens = [token for text in tokenized_texts for token in text]\n    token_counts = Counter(all_tokens)\n\n    # Filter tokens by minimum frequency\n    filtered_tokens = {token: count for token, count in token_counts.items() if count >= min_freq}\n\n    # Create the vocabulary with special tokens first\n    vocab = {token: idx for idx, token in enumerate(special_tokens)}\n    vocab.update({token: idx + len(vocab) for idx, token in enumerate(filtered_tokens.keys())})\n\n    return vocab, filtered_tokens\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:48:03.252817Z","iopub.execute_input":"2024-12-13T04:48:03.253144Z","iopub.status.idle":"2024-12-13T04:48:03.263936Z","shell.execute_reply.started":"2024-12-13T04:48:03.253109Z","shell.execute_reply":"2024-12-13T04:48:03.262907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:48:03.265084Z","iopub.execute_input":"2024-12-13T04:48:03.265412Z","iopub.status.idle":"2024-12-13T04:48:03.275785Z","shell.execute_reply.started":"2024-12-13T04:48:03.265388Z","shell.execute_reply":"2024-12-13T04:48:03.274893Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.texts[idx]), torch.tensor(self.labels[idx])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:48:03.276848Z","iopub.execute_input":"2024-12-13T04:48:03.277585Z","iopub.status.idle":"2024-12-13T04:48:03.286727Z","shell.execute_reply.started":"2024-12-13T04:48:03.277528Z","shell.execute_reply":"2024-12-13T04:48:03.28622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\ndata[\"Texts\"] = data[\"Paragraph\"].apply(\nlambda x: tokenizer.encode(x, truncation=True, max_length=256, padding=\"max_length\")\n)\ntexts = data['Texts'].tolist()\n\n# unique_topics = data['Topic'].unique()\n# topic_to_id = {topic: idx for idx, topic in enumerate(unique_topics)} \n\n# data['Topic'] = data['Topic'].map(topic_to_id)\nlabels = data['Topic'].tolist()\n\n# vocab, token_freqs = build_vocab(tokenized_texts, min_freq=1)\n\n# Hyperparameters and training setup\nmax_len = 15\nsrc_vocab_size = tokenizer.vocab_size   # Adjust based on your dataset's vocabulary size\nsrc_seq_len = 512                       # Maximum sequence length for a paragraph\nd_model = 256                           # Embedding size\nN = 6                                   # Number of encoder layers\nh = 8                                   # Number of attention heads\nd_ff = 1024                             # Feedforward network size\nnum_classes = 15                        # Number of topic classes\ndropout = 0.2                           # Dropout rate\nbatch_size = 64                         # Number of samples per batch\nepochs = 20                             # Number of epochs\nlearning_rate = 0.001                   # Learning rate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:48:03.2902Z","iopub.execute_input":"2024-12-13T04:48:03.291419Z","iopub.status.idle":"2024-12-13T04:48:11.472968Z","shell.execute_reply.started":"2024-12-13T04:48:03.291376Z","shell.execute_reply":"2024-12-13T04:48:11.472289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, Subset\n\nif __name__ == \"__main__\":\n    # Dummy data loader\n\n    dataset = TextDataset(texts, labels)\n\n    # Split the data into train, validation, and test sets (80% train, 10% validation, 10% test)\n    train_indices, test_val_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n    val_indices, test_indices = train_test_split(test_val_indices, test_size=0.5, random_state=42)\n\n    # Create subsets for train, validation, and test\n    train_dataset = Subset(dataset, train_indices)\n    val_dataset = Subset(dataset, val_indices)\n    test_dataset = Subset(dataset, test_indices)\n\n    # DataLoaders for train, validation, and test sets\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    # train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n    # texts, labels, test_size=0.2, random_state=42\n    # )\n    # val_texts, test_texts, val_labels, test_labels = train_test_split(\n    #     temp_texts, temp_labels, test_size=0.5, random_state=42\n    # )\n\n    # train_dataset = TextDataset(train_texts, train_labels)\n    # val_dataset = TextDataset(val_texts, val_labels)\n    # test_dataset = TextDataset(test_texts, test_labels)\n  \n    # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    # val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    # test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n        # Instantiate the model\n    model = TransformerEncoderClassifier(\n        src_vocab_size=src_vocab_size,\n        src_seq_len=src_seq_len,\n        d_model=d_model,\n        N=N,\n        h=h,\n        d_ff=d_ff,\n        num_classes=num_classes,\n        dropout=dropout\n    )\n\n\n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    # Dummy training loop\n    def train_model(model, train_loader, val_loader, num_epochs=10):\n        model.train()\n        for epoch in range(num_epochs):\n            # Training\n            for para, label in train_loader:\n                paragraphs, labels = para.to(device), label.to(device)\n                optimizer.zero_grad()\n                \n                # Create masks (assuming 0 is the PAD token)\n                src_mask = create_padding_mask(paragraphs, pad_token_idx=0)\n                \n                # Forward pass\n                outputs = model(paragraphs, src_mask)  # (batch, num_classes)\n                \n                # Compute loss\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n            \n            accuracy = lambda y_pred, y_true: (y_pred.argmax(1) == y_true).float().mean().item()\n            \n            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Accuracy: {accuracy(outputs, labels):.2f}\")\n            \n            # Validation step\n            model.eval()\n            val_loss = 0\n            val_accuracy = 0\n            with torch.no_grad():\n                for para, label in val_loader:\n                    paragraphs, labels = para.to(device), label.to(device)\n                    src_mask = create_padding_mask(paragraphs, pad_token_idx=0)\n                    outputs = model(paragraphs, src_mask)\n                    loss = criterion(outputs, labels)\n                    val_loss += loss.item()\n                    val_accuracy += accuracy(outputs, labels)\n\n            val_loss /= len(val_loader)\n            val_accuracy /= len(val_loader)\n\n            print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}\")\n            model.train()\n\n    # Train the model\n    train_model(model, train_loader, val_loader, num_epochs=epochs)\n    # Save the model's state dictionary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:48:11.473965Z","iopub.execute_input":"2024-12-13T04:48:11.474243Z","iopub.status.idle":"2024-12-13T05:11:28.183521Z","shell.execute_reply.started":"2024-12-13T04:48:11.474217Z","shell.execute_reply":"2024-12-13T05:11:28.182666Z"}},"outputs":[],"execution_count":null}]}